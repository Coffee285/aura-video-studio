<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Ollama Model Selection Feature | Aura Video Studio </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Ollama Model Selection Feature | Aura Video Studio ">
      
      
      <link rel="icon" href="../../favicon.ico">
      <link rel="stylesheet" href="../../public/docfx.min.css">
      <link rel="stylesheet" href="../../public/main.css">
      <meta name="docfx:navrel" content="../../toc.html">
      <meta name="docfx:tocrel" content="../../toc.html">
      
      <meta name="docfx:rel" content="../../">
      
      
      <meta name="docfx:docurl" content="https://github.com/Coffee285/aura-video-studio/blob/main/docs/providers/OLLAMA_MODEL_SELECTION.md/#L1">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../../index.html">
            <img id="logo" class="svg" src="../../logo.svg" alt="Aura Video Studio">
            Aura Video Studio
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">

      <div class="content">
        <div class="actionbar">

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="">
<h1 id="ollama-model-selection-feature">Ollama Model Selection Feature</h1>

<h2 id="overview">Overview</h2>
<p>This feature allows users to select any Ollama model available on their system instead of being locked to the default <code>llama3.1:8b-q4_k_m</code> model.</p>
<h2 id="user-guide">User Guide</h2>
<h3 id="selecting-an-ollama-model">Selecting an Ollama Model</h3>
<ol>
<li><p><strong>Navigate to Settings</strong></p>
<ul>
<li>Open Aura Video Studio</li>
<li>Go to Settings → Providers → Provider Paths</li>
</ul>
</li>
<li><p><strong>Configure Ollama URL</strong> (if not already set)</p>
<ul>
<li>Ensure the &quot;Ollama URL&quot; field is set to where Ollama is running</li>
<li>Default: <code>http://127.0.0.1:11434</code></li>
<li>Click &quot;Test Connection&quot; to verify Ollama is accessible</li>
</ul>
</li>
<li><p><strong>Select a Model</strong></p>
<ul>
<li>Find the &quot;Ollama Model&quot; section below the Ollama URL field</li>
<li>Click the &quot;Refresh Models&quot; button to load available models from your Ollama installation</li>
<li>A dropdown will populate with all available models, showing:
<ul>
<li>Model name (e.g., <code>llama3.1:8b-q4_k_m</code>)</li>
<li>Model size in GB (e.g., <code>(4.92 GB)</code>)</li>
</ul>
</li>
<li>Select your desired model from the dropdown</li>
<li>The selection is automatically saved</li>
</ul>
</li>
<li><p><strong>Use the Selected Model</strong></p>
<ul>
<li>The next time you generate a video, the selected model will be used</li>
<li>You can change the model at any time and it will be used for subsequent generations</li>
</ul>
</li>
</ol>
<h2 id="technical-details">Technical Details</h2>
<h3 id="api-endpoints">API Endpoints</h3>
<h4 id="get-available-models">Get Available Models</h4>
<pre><code>GET /api/engines/ollama/models?url={ollamaUrl}
</code></pre>
<p><strong>Response:</strong></p>
<pre><code class="lang-json">{
  &quot;models&quot;: [
    {
      &quot;name&quot;: &quot;llama3.1:8b-q4_k_m&quot;,
      &quot;size&quot;: 5278359952,
      &quot;sizeGB&quot;: 4.92,
      &quot;modifiedAt&quot;: &quot;2024-10-15T10:30:00Z&quot;,
      &quot;digest&quot;: &quot;sha256:abc123...&quot;
    }
  ],
  &quot;baseUrl&quot;: &quot;http://127.0.0.1:11434&quot;
}
</code></pre>
<h4 id="get-current-model-setting">Get Current Model Setting</h4>
<pre><code>GET /api/settings/ollama/model
</code></pre>
<p><strong>Response:</strong></p>
<pre><code class="lang-json">{
  &quot;success&quot;: true,
  &quot;model&quot;: &quot;llama3.1:8b-q4_k_m&quot;
}
</code></pre>
<h4 id="set-model">Set Model</h4>
<pre><code>POST /api/settings/ollama/model
Content-Type: application/json

{
  &quot;model&quot;: &quot;llama3.1:8b-q4_k_m&quot;
}
</code></pre>
<p><strong>Response:</strong></p>
<pre><code class="lang-json">{
  &quot;success&quot;: true,
  &quot;message&quot;: &quot;Ollama model updated successfully&quot;,
  &quot;model&quot;: &quot;llama3.1:8b-q4_k_m&quot;
}
</code></pre>
<h3 id="configuration-storage">Configuration Storage</h3>
<p>The selected model is stored in <code>AuraData/settings.json</code>:</p>
<pre><code class="lang-json">{
  &quot;ollamaModel&quot;: &quot;llama3.1:8b-q4_k_m&quot;
}
</code></pre>
<h3 id="default-behavior">Default Behavior</h3>
<ul>
<li><strong>Default Model</strong>: <code>llama3.1:8b-q4_k_m</code></li>
<li>If no model is configured, the default is used automatically</li>
<li>Backward compatible with existing installations</li>
</ul>
<h2 id="hardware-specific-recommendations">Hardware-Specific Recommendations</h2>
<p>Aura Video Studio provides intelligent model recommendations based on your hardware:</p>
<h3 id="ram-based-recommendations">RAM-Based Recommendations</h3>
<p><strong>Less than 8GB RAM:</strong></p>
<ul>
<li><strong>Recommended:</strong> Models 3B or smaller</li>
<li><strong>Example:</strong> <code>llama3.2:3b</code> or <code>phi3:mini</code></li>
<li><strong>Reason:</strong> Smaller models reduce memory pressure and prevent system slowdowns</li>
</ul>
<p><strong>8GB RAM:</strong></p>
<ul>
<li><strong>Recommended:</strong> <code>llama3.1:8b-q4_k_m</code> (quantized 4-bit)</li>
<li><strong>Alternative:</strong> <code>mistral:7b-q4</code></li>
<li><strong>Reason:</strong> 4-bit quantization provides good balance of quality and memory usage</li>
</ul>
<p><strong>16GB+ RAM:</strong></p>
<ul>
<li><strong>Recommended:</strong> <code>llama3.1:8b-q4_k_m</code> (comfortable)</li>
<li><strong>Alternative:</strong> <code>llama3.1:8b</code> (higher quality, more memory)</li>
<li><strong>Reason:</strong> Can run 8B models comfortably with room for other applications</li>
</ul>
<p><strong>32GB+ RAM:</strong></p>
<ul>
<li><strong>Recommended:</strong> <code>llama3.1:70b-q4_k_m</code> (if you have GPU)</li>
<li><strong>Alternative:</strong> <code>llama3.1:8b</code> or <code>mixtral:8x7b-q4</code></li>
<li><strong>Reason:</strong> Can run larger models or multiple models simultaneously</li>
</ul>
<h3 id="gpu-acceleration">GPU Acceleration</h3>
<p>If you have a GPU with 8GB+ VRAM:</p>
<ul>
<li>Models can run significantly faster with GPU acceleration</li>
<li>Configure Ollama to use GPU: Ollama automatically detects and uses CUDA-capable GPUs</li>
<li>Check GPU usage: <code>nvidia-smi</code> (NVIDIA) or task manager</li>
</ul>
<p><strong>GPU Model Recommendations:</strong></p>
<ul>
<li><strong>8GB VRAM:</strong> <code>llama3.1:8b-q4_k_m</code></li>
<li><strong>12GB VRAM:</strong> <code>llama3.1:8b</code> or <code>llama2:13b-q4</code></li>
<li><strong>24GB+ VRAM:</strong> <code>llama3.1:70b-q4_k_m</code> (for best quality)</li>
</ul>
<h3 id="quality-vs-speed-trade-offs">Quality vs. Speed Trade-offs</h3>
<p><strong>Fast (Low Latency):</strong></p>
<ul>
<li><strong>Models:</strong> <code>phi3:mini</code>, <code>llama3.2:3b</code>, <code>gemma:2b</code></li>
<li><strong>Use Case:</strong> Quick iterations, testing, low-end hardware</li>
<li><strong>Expected Quality:</strong> Good for short scripts, acceptable grammar</li>
</ul>
<p><strong>Balanced (Recommended):</strong></p>
<ul>
<li><strong>Models:</strong> <code>llama3.1:8b-q4_k_m</code>, <code>mistral:7b-q4</code></li>
<li><strong>Use Case:</strong> Production videos, most users</li>
<li><strong>Expected Quality:</strong> Excellent grammar, creative, engaging scripts</li>
</ul>
<p><strong>High Quality (Slower):</strong></p>
<ul>
<li><strong>Models:</strong> <code>llama3.1:70b-q4_k_m</code>, <code>mixtral:8x7b-instruct</code></li>
<li><strong>Use Case:</strong> Professional content, maximum quality</li>
<li><strong>Expected Quality:</strong> Near-human writing, excellent coherence</li>
</ul>
<h3 id="keep-alive-configuration">Keep-Alive Configuration</h3>
<p>To reduce model loading time between requests:</p>
<ol>
<li>Open Aura Video Studio Settings</li>
<li>Navigate to Provider Settings → Ollama</li>
<li>Set &quot;Keep Alive&quot; to a duration that fits your workflow:
<ul>
<li><strong>5 minutes</strong> (300s): For occasional use</li>
<li><strong>30 minutes</strong> (1800s): For active editing sessions</li>
<li><strong>Unlimited</strong> (-1): Keep model loaded always (uses RAM continuously)</li>
</ul>
</li>
</ol>
<p><strong>Note:</strong> Keeping models loaded uses RAM but eliminates load time (5-30 seconds per request).</p>
<h3 id="checking-ollama-status">Checking Ollama Status</h3>
<p>Aura Video Studio includes an Ollama status checker:</p>
<ol>
<li>Open Settings → Providers → Offline Status</li>
<li>Click &quot;Check Ollama&quot;</li>
<li>View:
<ul>
<li>Whether Ollama is running</li>
<li>List of installed models</li>
<li>Hardware-specific recommendations</li>
<li>Model sizes and memory requirements</li>
</ul>
</li>
</ol>
<h3 id="performance-tips">Performance Tips</h3>
<ol>
<li><strong>First Run:</strong> The first request to Ollama loads the model into memory (5-30s delay). Subsequent requests are fast.</li>
<li><strong>Concurrent Requests:</strong> Ollama handles one request at a time per model.</li>
<li><strong>Memory Management:</strong> If system becomes slow, use smaller models or increase keep-alive to prevent constant loading.</li>
<li><strong>SSD vs HDD:</strong> Model loading is much faster from SSD storage.</li>
</ol>
<h3 id="advanced-model-quantization">Advanced: Model Quantization</h3>
<p>Quantization reduces model size and memory usage with minimal quality loss:</p>
<ul>
<li><strong>q4_k_m:</strong> 4-bit quantization, good balance (recommended)</li>
<li><strong>q5_k_m:</strong> 5-bit, slightly better quality, more memory</li>
<li><strong>q8_0:</strong> 8-bit, near-original quality, much more memory</li>
<li><strong>f16/f32:</strong> Full precision, highest quality, maximum memory</li>
</ul>
<p><strong>Example:</strong></p>
<ul>
<li><code>llama3.1:8b</code> (16GB RAM required)</li>
<li><code>llama3.1:8b-q4_k_m</code> (5GB RAM required) ← Recommended default</li>
</ul>
<h2 id="benefits">Benefits</h2>
<h3 id="performance-optimization">Performance Optimization</h3>
<ul>
<li><strong>Smaller Models</strong>: Use lighter models like <code>llama3.2:3b</code> for faster generation on limited hardware</li>
<li><strong>Larger Models</strong>: Use more powerful models like <code>llama3.1:70b</code> for higher quality when you have the resources</li>
</ul>
<h3 id="use-case-flexibility">Use Case Flexibility</h3>
<ul>
<li><strong>Specialized Models</strong>: Use models fine-tuned for specific content types (technical, creative, educational)</li>
<li><strong>Language Support</strong>: Choose models optimized for different languages</li>
<li><strong>Quality vs Speed</strong>: Balance generation quality with speed based on your needs</li>
</ul>
<h3 id="hardware-compatibility">Hardware Compatibility</h3>
<ul>
<li>Match model size to your available VRAM</li>
<li>Optimize for your specific hardware configuration</li>
</ul>
<h2 id="examples">Examples</h2>
<h3 id="common-model-configurations">Common Model Configurations</h3>
<h4 id="fast-generation-low-resource">Fast Generation (Low Resource)</h4>
<pre><code>Model: llama3.2:3b
Size: ~2 GB
Use case: Quick drafts, testing, low-end hardware
</code></pre>
<h4 id="balanced-default">Balanced (Default)</h4>
<pre><code>Model: llama3.1:8b-q4_k_m
Size: ~5 GB
Use case: General purpose, good balance of speed and quality
</code></pre>
<h4 id="high-quality">High Quality</h4>
<pre><code>Model: llama3.1:70b
Size: ~40 GB
Use case: Production content, best quality, requires powerful hardware
</code></pre>
<h4 id="specialized">Specialized</h4>
<pre><code>Model: codellama:13b
Size: ~7 GB
Use case: Technical/coding content generation
</code></pre>
<h2 id="troubleshooting">Troubleshooting</h2>
<h3 id="no-models-appearing">No Models Appearing</h3>
<ul>
<li>Ensure Ollama is running: <code>ollama serve</code></li>
<li>Check the Ollama URL is correct</li>
<li>Click &quot;Test Connection&quot; to verify connectivity</li>
<li>Pull models if none are installed: <code>ollama pull llama3.1</code></li>
</ul>
<h3 id="model-not-loading">Model Not Loading</h3>
<ul>
<li>Verify the model exists in Ollama: <code>ollama list</code></li>
<li>Check available VRAM/RAM for the model size</li>
<li>Try a smaller model if you have resource constraints</li>
</ul>
<h3 id="generation-fails">Generation Fails</h3>
<ul>
<li>Confirm the selected model is still available in Ollama</li>
<li>Check Ollama logs for errors</li>
<li>Try switching to the default model</li>
<li>Ensure Ollama service is running during generation</li>
</ul>
<h2 id="implementation-notes">Implementation Notes</h2>
<h3 id="files-modified">Files Modified</h3>
<p><strong>Backend:</strong></p>
<ul>
<li><code>Aura.Api/Controllers/EnginesController.cs</code> - Added models endpoint</li>
<li><code>Aura.Api/Controllers/SettingsController.cs</code> - Added get/set model endpoints</li>
<li><code>Aura.Core/Configuration/ProviderSettings.cs</code> - Added model settings methods</li>
<li><code>Aura.Core/Orchestrator/LlmProviderFactory.cs</code> - Uses configured model</li>
</ul>
<p><strong>Frontend:</strong></p>
<ul>
<li><code>Aura.Web/src/types/api-v1.ts</code> - Added Ollama model types</li>
<li><code>Aura.Web/src/types/settings.ts</code> - Added model to settings</li>
<li><code>Aura.Web/src/pages/SettingsPage.tsx</code> - Added UI for model selection</li>
</ul>
<p><strong>Tests:</strong></p>
<ul>
<li><code>Aura.Tests/ProviderSettingsTests.cs</code> - Added unit tests for model settings</li>
</ul>
<h3 id="backward-compatibility">Backward Compatibility</h3>
<p>This feature is fully backward compatible:</p>
<ul>
<li>Existing installations continue using <code>llama3.1:8b-q4_k_m</code> by default</li>
<li>No breaking changes to existing workflows</li>
<li>Settings file is optional (defaults are used if not present)</li>
</ul>
<h2 id="future-enhancements">Future Enhancements</h2>
<p>Potential improvements for future releases:</p>
<ul>
<li>Show model capabilities/tags in UI (context length, specializations)</li>
<li>Display recommended models based on user's hardware profile</li>
<li>Add model performance metrics (speed, quality ratings)</li>
<li>Support for model groups/categories</li>
<li>Quick model switching during workflow without going to settings</li>
</ul>

</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/Coffee285/aura-video-studio/blob/main/docs/providers/OLLAMA_MODEL_SELECTION.md/#L1" class="edit-link">Edit this page</a>
        </div>

        <div class="next-article d-print-none border-top" id="nextArticle"></div>

      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          © 2025 Aura Video Studio. Documentation built with DocFX.
        </div>
      </div>
    </footer>
  </body>
</html>
