<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
      <title>Ollama Model Recommendations for Translation | Aura Video Studio </title>
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      <meta name="title" content="Ollama Model Recommendations for Translation | Aura Video Studio ">
      
      
      <link rel="icon" href="../../favicon.ico">
      <link rel="stylesheet" href="../../public/docfx.min.css">
      <link rel="stylesheet" href="../../public/main.css">
      <meta name="docfx:navrel" content="../../toc.html">
      <meta name="docfx:tocrel" content="../../toc.html">
      
      <meta name="docfx:rel" content="../../">
      
      
      <meta name="docfx:docurl" content="https://github.com/Coffee285/aura-video-studio/blob/main/docs/user-guide/TRANSLATION_OLLAMA_MODELS.md/#L1">
      <meta name="loc:inThisArticle" content="In this article">
      <meta name="loc:searchResultsCount" content="{count} results for &quot;{query}&quot;">
      <meta name="loc:searchNoResults" content="No results for &quot;{query}&quot;">
      <meta name="loc:tocFilter" content="Filter by title">
      <meta name="loc:nextArticle" content="Next">
      <meta name="loc:prevArticle" content="Previous">
      <meta name="loc:themeLight" content="Light">
      <meta name="loc:themeDark" content="Dark">
      <meta name="loc:themeAuto" content="Auto">
      <meta name="loc:changeTheme" content="Change theme">
      <meta name="loc:copy" content="Copy">
      <meta name="loc:downloadPdf" content="Download PDF">

      <script type="module" src="./../../public/docfx.min.js"></script>

      <script>
        const theme = localStorage.getItem('theme') || 'auto'
        document.documentElement.setAttribute('data-bs-theme', theme === 'auto' ? (window.matchMedia('(prefers-color-scheme: dark)').matches ? 'dark' : 'light') : theme)
      </script>

  </head>

  <body class="tex2jax_ignore" data-layout="" data-yaml-mime="">
    <header class="bg-body border-bottom">
      <nav id="autocollapse" class="navbar navbar-expand-md" role="navigation">
        <div class="container-xxl flex-nowrap">
          <a class="navbar-brand" href="../../index.html">
            <img id="logo" class="svg" src="../../logo.svg" alt="Aura Video Studio">
            Aura Video Studio
          </a>
          <button class="btn btn-lg d-md-none border-0" type="button" data-bs-toggle="collapse" data-bs-target="#navpanel" aria-controls="navpanel" aria-expanded="false" aria-label="Toggle navigation">
            <i class="bi bi-three-dots"></i>
          </button>
          <div class="collapse navbar-collapse" id="navpanel">
            <div id="navbar">
              <form class="search" role="search" id="search">
                <i class="bi bi-search"></i>
                <input class="form-control" id="search-query" type="search" disabled placeholder="Search" autocomplete="off" aria-label="Search">
              </form>
            </div>
          </div>
        </div>
      </nav>
    </header>

    <main class="container-xxl">

      <div class="content">
        <div class="actionbar">

          <nav id="breadcrumb"></nav>
        </div>

        <article data-uid="">
<h1 id="ollama-model-recommendations-for-translation">Ollama Model Recommendations for Translation</h1>

<p>This guide helps you select the best Ollama model for translation tasks in Aura Video Studio.</p>
<h2 id="best-models-for-translation">Best Models for Translation</h2>
<p>Based on testing with the Aura Video Studio localization feature:</p>
<h3 id="tier-1-excellent-recommended">Tier 1: Excellent (Recommended)</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Ollama Tag</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>llama3.1</strong></td>
<td><code>llama3.1:8b</code></td>
<td>Best overall balance of quality and speed. Produces clean output with minimal artifacts.</td>
</tr>
<tr>
<td><strong>mistral</strong></td>
<td><code>mistral:7b</code></td>
<td>Very clean output, minimal artifacts. Excellent for European languages.</td>
</tr>
<tr>
<td><strong>gemma2</strong></td>
<td><code>gemma2:9b</code></td>
<td>High accuracy, good with cultural nuances. Great for nuanced translations.</td>
</tr>
</tbody>
</table>
<h3 id="tier-2-good">Tier 2: Good</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Ollama Tag</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>llama3.2</strong></td>
<td><code>llama3.2:3b</code></td>
<td>Fast, acceptable quality for simple translations. Good for quick iterations.</td>
</tr>
<tr>
<td><strong>phi3</strong></td>
<td><code>phi3:3.8b</code></td>
<td>Compact, decent for technical content. Lower memory requirements.</td>
</tr>
</tbody>
</table>
<h3 id="tier-3-fair-use-with-caution">Tier 3: Fair (Use with Caution)</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Ollama Tag</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>llama2</strong></td>
<td><code>llama2:7b</code></td>
<td>Older model, tends to generate verbose output with JSON artifacts.</td>
</tr>
<tr>
<td><strong>codellama</strong></td>
<td><code>codellama:7b</code></td>
<td>Optimized for code, not ideal for natural language translation.</td>
</tr>
</tbody>
</table>
<h2 id="installation">Installation</h2>
<p>Install your preferred model using Ollama:</p>
<pre><code class="lang-bash"># Install recommended model
ollama pull llama3.1

# Verify installation
ollama list

# Run the model
ollama run llama3.1
</code></pre>
<h2 id="model-parameters-for-translation">Model Parameters for Translation</h2>
<p>When configuring Ollama models for translation in Aura, consider these parameters:</p>
<pre><code class="lang-json">{
  &quot;temperature&quot;: 0.3,
  &quot;top_p&quot;: 0.9,
  &quot;repeat_penalty&quot;: 1.1,
  &quot;num_ctx&quot;: 4096
}
</code></pre>
<h3 id="parameter-guidelines">Parameter Guidelines</h3>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Recommended Value</th>
<th>Purpose</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>temperature</code></td>
<td>0.3</td>
<td>Lower values produce more consistent, literal translations</td>
</tr>
<tr>
<td><code>top_p</code></td>
<td>0.9</td>
<td>Nucleus sampling for balanced output variety</td>
</tr>
<tr>
<td><code>repeat_penalty</code></td>
<td>1.1</td>
<td>Reduces repetition in longer translations</td>
</tr>
<tr>
<td><code>num_ctx</code></td>
<td>4096</td>
<td>Context window for handling longer texts</td>
</tr>
</tbody>
</table>
<h2 id="troubleshooting-by-model">Troubleshooting by Model</h2>
<h3 id="llama2-7b-generates-structured-json-output">llama2 (7B) generates structured JSON output</h3>
<p><strong>Problem</strong>: The model returns JSON objects like <code>{&quot;title&quot;: &quot;...&quot;, &quot;translation&quot;: &quot;...&quot;}</code> instead of plain translated text.</p>
<p><strong>Solution</strong>:</p>
<ul>
<li>Upgrade to llama3.1:8b or mistral:7b for cleaner output</li>
<li>The automatic cleanup in Aura handles this, but Tier 1 models rarely produce such artifacts</li>
</ul>
<h3 id="model-returns-translation-prefix">Model returns &quot;Translation:&quot; prefix</h3>
<p><strong>Problem</strong>: Output begins with prefixes like &quot;Translation:&quot;, &quot;Here is the translation:&quot;, etc.</p>
<p><strong>Solution</strong>:</p>
<ul>
<li>Automatic cleanup is applied by Aura Video Studio</li>
<li>For best results, consider switching to a Tier 1 model</li>
<li>Check the translation quality metrics in the result to monitor this issue</li>
</ul>
<h3 id="translation-takes-longer-than-30-seconds">Translation takes longer than 30 seconds</h3>
<p><strong>Problem</strong>: Translations are slow, especially for longer texts.</p>
<p><strong>Solutions</strong>:</p>
<ol>
<li>Use a smaller model (llama3.2 3B) for faster processing</li>
<li>Upgrade hardware (more RAM, faster CPU/GPU)</li>
<li>Enable GPU acceleration if using an NVIDIA GPU</li>
<li>Reduce batch size for very long texts</li>
</ol>
<h3 id="model-produces-unusually-long-output">Model produces unusually long output</h3>
<p><strong>Problem</strong>: Translation is 3x or more the length of the source text.</p>
<p><strong>Causes</strong>:</p>
<ul>
<li>Model generating explanations or tutorials instead of translations</li>
<li>Verbose output patterns in older models</li>
</ul>
<p><strong>Solutions</strong>:</p>
<ul>
<li>Upgrade to Tier 1 models (llama3.1, mistral, gemma2)</li>
<li>Check the <code>LengthRatio</code> in translation metrics</li>
<li>Report unusual patterns in the quality issues</li>
</ul>
<h2 id="performance-benchmarks">Performance Benchmarks</h2>
<p>Approximate performance on consumer hardware (16GB RAM, no GPU acceleration):</p>
<table>
<thead>
<tr>
<th>Model</th>
<th>Speed (tokens/sec)</th>
<th>Quality Grade</th>
<th>Artifacts?</th>
<th>Memory Usage</th>
</tr>
</thead>
<tbody>
<tr>
<td>llama3.1 8B</td>
<td>25-40</td>
<td>Excellent</td>
<td>Rare</td>
<td>~8GB</td>
</tr>
<tr>
<td>mistral 7B</td>
<td>30-45</td>
<td>Excellent</td>
<td>Rare</td>
<td>~6GB</td>
</tr>
<tr>
<td>gemma2 9B</td>
<td>20-35</td>
<td>Excellent</td>
<td>Rare</td>
<td>~9GB</td>
</tr>
<tr>
<td>llama3.2 3B</td>
<td>50-70</td>
<td>Good</td>
<td>Occasional</td>
<td>~4GB</td>
</tr>
<tr>
<td>phi3 3.8B</td>
<td>45-60</td>
<td>Good</td>
<td>Occasional</td>
<td>~4GB</td>
</tr>
<tr>
<td>llama2 7B</td>
<td>25-40</td>
<td>Fair</td>
<td>Common</td>
<td>~6GB</td>
</tr>
</tbody>
</table>
<h3 id="with-gpu-acceleration-nvidia-rtx">With GPU Acceleration (NVIDIA RTX)</h3>
<table>
<thead>
<tr>
<th>Model</th>
<th>Speed (tokens/sec)</th>
<th>Notes</th>
</tr>
</thead>
<tbody>
<tr>
<td>llama3.1 8B</td>
<td>80-120</td>
<td>Recommended for production</td>
</tr>
<tr>
<td>mistral 7B</td>
<td>90-130</td>
<td>Fastest Tier 1 model</td>
</tr>
<tr>
<td>gemma2 9B</td>
<td>70-100</td>
<td>Best quality per token</td>
</tr>
</tbody>
</table>
<h2 id="quality-metrics-explained">Quality Metrics Explained</h2>
<p>Aura Video Studio tracks translation quality metrics automatically:</p>
<h3 id="translationqualitygrade">TranslationQualityGrade</h3>
<table>
<thead>
<tr>
<th>Grade</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Excellent</strong></td>
<td>Clean output, appropriate length, fast processing</td>
</tr>
<tr>
<td><strong>Good</strong></td>
<td>Minor issues that were cleaned up automatically</td>
</tr>
<tr>
<td><strong>Fair</strong></td>
<td>Multiple issues detected, may need manual review</td>
</tr>
<tr>
<td><strong>Poor</strong></td>
<td>Significant problems, output may need regeneration</td>
</tr>
</tbody>
</table>
<h3 id="key-metrics">Key Metrics</h3>
<table>
<thead>
<tr>
<th>Metric</th>
<th>Description</th>
<th>Ideal Range</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>LengthRatio</code></td>
<td>Translation length / Source length</td>
<td>0.5 - 2.5</td>
</tr>
<tr>
<td><code>HasStructuredArtifacts</code></td>
<td>JSON/XML detected in output</td>
<td>false</td>
</tr>
<tr>
<td><code>HasUnwantedPrefixes</code></td>
<td>&quot;Translation:&quot; etc. in output</td>
<td>false</td>
</tr>
<tr>
<td><code>TranslationTimeSeconds</code></td>
<td>Time to complete</td>
<td>&lt; 30s</td>
</tr>
</tbody>
</table>
<h2 id="recommended-workflow">Recommended Workflow</h2>
<ol>
<li><strong>Start with llama3.1</strong> for general translation tasks</li>
<li><strong>Monitor quality metrics</strong> in the translation results</li>
<li><strong>Switch to mistral</strong> if you notice frequent artifacts</li>
<li><strong>Use llama3.2</strong> when speed is critical and text is simple</li>
<li><strong>Avoid llama2</strong> for production translations</li>
</ol>
<h2 id="getting-help">Getting Help</h2>
<p>If you experience issues with translation quality:</p>
<ol>
<li>Check the translation metrics in the API response</li>
<li>Review the quality issues list for specific problems</li>
<li>Try a different Tier 1 model</li>
<li>Report persistent issues on GitHub</li>
</ol>
<h2 id="related-documentation">Related Documentation</h2>
<ul>
<li><a href="TRANSLATION_USER_GUIDE.html">Translation User Guide</a></li>
<li><a href="PROVIDER_INTEGRATION_GUIDE.html">Provider Integration Guide</a></li>
<li><a href="LOCAL_PROVIDERS_SETUP.html">Local Providers Setup</a></li>
</ul>

</article>

        <div class="contribution d-print-none">
          <a href="https://github.com/Coffee285/aura-video-studio/blob/main/docs/user-guide/TRANSLATION_OLLAMA_MODELS.md/#L1" class="edit-link">Edit this page</a>
        </div>

        <div class="next-article d-print-none border-top" id="nextArticle"></div>

      </div>

      <div class="affix">
        <nav id="affix"></nav>
      </div>
    </main>

    <div class="container-xxl search-results" id="search-results"></div>

    <footer class="border-top text-secondary">
      <div class="container-xxl">
        <div class="flex-fill">
          Â© 2025 Aura Video Studio. Documentation built with DocFX.
        </div>
      </div>
    </footer>
  </body>
</html>
